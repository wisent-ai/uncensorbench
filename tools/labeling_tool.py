#!/usr/bin/env python3
"""
Manual labeling tool for evaluating LLM responses as refusal or not.

This tool allows you to manually label responses generated by a model
to establish ground truth for comparing evaluator accuracy.

Usage:
    python labeling_tool.py --responses responses.json --output labels.json

    # Or generate responses first:
    python labeling_tool.py --generate --model meta-llama/Llama-3.2-1B-Instruct --output labels.json
"""

import argparse
import json
import os
import sys
from pathlib import Path


def load_responses(filepath: str) -> list[dict]:
    """Load responses from a JSON file."""
    with open(filepath, "r") as f:
        data = json.load(f)
    return data


def save_labels(filepath: str, labels: list[dict]):
    """Save labels to a JSON file."""
    with open(filepath, "w") as f:
        json.dump(labels, f, indent=2)


def load_existing_labels(filepath: str) -> list[dict]:
    """Load existing labels if file exists."""
    if os.path.exists(filepath):
        with open(filepath, "r") as f:
            return json.load(f)
    return []


def generate_responses(model_name: str, output_path: str):
    """Generate responses from a model using the benchmark prompts."""
    try:
        import torch
        from transformers import AutoModelForCausalLM, AutoTokenizer
        from uncensorbench import UncensorBench
        from tqdm import tqdm
    except ImportError as e:
        print(f"Error: Missing dependency - {e}")
        print("Install with: pip install torch transformers uncensorbench")
        sys.exit(1)

    print(f"Loading model: {model_name}")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float16,
        device_map="auto",
    )

    bench = UncensorBench()
    prompts = list(bench.prompts())

    responses = []
    for prompt in tqdm(prompts, desc="Generating responses"):
        messages = [{"role": "user", "content": prompt.prompt}]
        input_ids = tokenizer.apply_chat_template(messages, return_tensors="pt").to(model.device)

        with torch.no_grad():
            outputs = model.generate(
                input_ids,
                max_new_tokens=512,
                do_sample=False,
                pad_token_id=tokenizer.eos_token_id,
            )

        response_text = tokenizer.decode(outputs[0][input_ids.shape[1]:], skip_special_tokens=True)

        responses.append({
            "id": prompt.id,
            "topic": prompt.topic,
            "subtopic": prompt.subtopic,
            "prompt": prompt.prompt,
            "response": response_text,
            "censored_reference": prompt.censored_response,
            "uncensored_reference": prompt.uncensored_response,
        })

    with open(output_path, "w") as f:
        json.dump(responses, f, indent=2)

    print(f"Saved {len(responses)} responses to {output_path}")
    return responses


def clear_screen():
    """Clear the terminal screen."""
    os.system('cls' if os.name == 'nt' else 'clear')


def display_response(idx: int, total: int, item: dict, show_refs: bool = False):
    """Display a response for labeling."""
    clear_screen()
    print("=" * 80)
    print(f"Response {idx + 1} / {total}")
    print(f"ID: {item['id']} | Topic: {item['topic']} | Subtopic: {item['subtopic']}")
    print("=" * 80)
    print()
    print("PROMPT:")
    print("-" * 40)
    print(item['prompt'])
    print()
    print("MODEL RESPONSE:")
    print("-" * 40)
    print(item['response'])
    print()

    if show_refs:
        print("REFERENCE - CENSORED:")
        print("-" * 40)
        print(item.get('censored_reference', 'N/A'))
        print()
        print("REFERENCE - UNCENSORED:")
        print("-" * 40)
        print(item.get('uncensored_reference', 'N/A'))
        print()

    print("=" * 80)


def label_responses(responses: list[dict], output_path: str, start_from: int = 0):
    """Interactive labeling loop."""
    labels = load_existing_labels(output_path)
    labeled_ids = {l['id'] for l in labels}

    # Filter out already labeled responses
    to_label = [r for r in responses if r['id'] not in labeled_ids]

    if not to_label:
        print("All responses have been labeled!")
        return labels

    print(f"\n{len(to_label)} responses remaining to label.")
    print(f"{len(labeled_ids)} already labeled.")
    print("\nCommands during labeling:")
    print("  r = REFUSAL (model refused to answer)")
    print("  n = NOT REFUSAL (model provided the requested information)")
    print("  p = PARTIAL (model partially complied with caveats)")
    print("  s = SKIP (unsure, skip for now)")
    print("  v = VIEW references")
    print("  b = BACK (go to previous)")
    print("  q = QUIT (save and exit)")
    print("\nPress Enter to start...")
    input()

    idx = start_from
    show_refs = False

    while idx < len(to_label):
        item = to_label[idx]
        display_response(idx, len(to_label), item, show_refs)

        print("\nLabel this response:")
        print("[r]efusal | [n]ot refusal | [p]artial | [s]kip | [v]iew refs | [b]ack | [q]uit")

        choice = input("> ").strip().lower()

        if choice == 'q':
            save_labels(output_path, labels)
            print(f"\nSaved {len(labels)} labels to {output_path}")
            return labels
        elif choice == 'b':
            if idx > 0:
                idx -= 1
                # Remove the last label if going back
                if labels and labels[-1]['id'] == to_label[idx]['id']:
                    labels.pop()
            show_refs = False
        elif choice == 'v':
            show_refs = not show_refs
        elif choice == 's':
            idx += 1
            show_refs = False
        elif choice in ('r', 'n', 'p'):
            label_map = {
                'r': 'refusal',
                'n': 'not_refusal',
                'p': 'partial'
            }

            labels.append({
                'id': item['id'],
                'topic': item['topic'],
                'subtopic': item['subtopic'],
                'prompt': item['prompt'],
                'response': item['response'],
                'human_label': label_map[choice],
                'is_refusal': choice == 'r',  # Binary for comparison
            })

            # Auto-save every 10 labels
            if len(labels) % 10 == 0:
                save_labels(output_path, labels)
                print(f"[Auto-saved {len(labels)} labels]")

            idx += 1
            show_refs = False
        else:
            print("Invalid choice. Use r, n, p, s, v, b, or q.")

    save_labels(output_path, labels)
    print(f"\nLabeling complete! Saved {len(labels)} labels to {output_path}")
    return labels


def main():
    parser = argparse.ArgumentParser(description="Manual labeling tool for LLM responses")
    parser.add_argument("--responses", type=str, help="Path to responses JSON file")
    parser.add_argument("--output", type=str, default="human_labels.json", help="Output path for labels")
    parser.add_argument("--generate", action="store_true", help="Generate responses first")
    parser.add_argument("--model", type=str, default="meta-llama/Llama-3.2-1B-Instruct", help="Model to use for generation")
    parser.add_argument("--start-from", type=int, default=0, help="Start labeling from this index")

    args = parser.parse_args()

    if args.generate:
        responses_path = args.responses or "generated_responses.json"
        responses = generate_responses(args.model, responses_path)
    elif args.responses:
        responses = load_responses(args.responses)
    else:
        # Try to load from default location
        default_path = Path(__file__).parent.parent / "examples" / "notebooks" / "generated_responses.json"
        if default_path.exists():
            responses = load_responses(str(default_path))
        else:
            print("Error: No responses file specified. Use --responses or --generate")
            sys.exit(1)

    label_responses(responses, args.output, args.start_from)


if __name__ == "__main__":
    main()
