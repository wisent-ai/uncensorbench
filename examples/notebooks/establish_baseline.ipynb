{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UncensorBench: Establish Model Baseline\n",
    "\n",
    "This notebook runs UncensorBench on a model and submits the results to the public leaderboard on HuggingFace.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, install the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q uncensorbench transformers torch huggingface_hub sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set the model you want to evaluate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the model to evaluate\n",
    "MODEL_NAME = \"meta-llama/Llama-3.2-1B-Instruct\"  # Change this to your model\n",
    "\n",
    "# Optional: HuggingFace token for pushing to leaderboard (set to None to skip)\n",
    "HF_TOKEN = None  # Or set your token: \"hf_...\"\n",
    "\n",
    "# Generation parameters\n",
    "MAX_NEW_TOKENS = 512\n",
    "TEMPERATURE = 0.7\n",
    "DEVICE = \"auto\"  # auto, cuda, mps, cpu\n",
    "DTYPE = \"auto\"   # auto, float16, bfloat16, float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "print(f\"Loading model: {MODEL_NAME}\")\n",
    "\n",
    "# Determine dtype\n",
    "if DTYPE == \"auto\":\n",
    "    if torch.cuda.is_available():\n",
    "        torch_dtype = torch.float16\n",
    "    elif torch.backends.mps.is_available():\n",
    "        torch_dtype = torch.float16\n",
    "    else:\n",
    "        torch_dtype = torch.float32\n",
    "elif DTYPE == \"float16\":\n",
    "    torch_dtype = torch.float16\n",
    "elif DTYPE == \"bfloat16\":\n",
    "    torch_dtype = torch.bfloat16\n",
    "else:\n",
    "    torch_dtype = torch.float32\n",
    "\n",
    "# Determine device\n",
    "if DEVICE == \"auto\":\n",
    "    if torch.cuda.is_available():\n",
    "        device_map = \"auto\"\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device_map = \"mps\"\n",
    "    else:\n",
    "        device_map = \"cpu\"\n",
    "else:\n",
    "    device_map = DEVICE\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device_map=device_map,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "print(f\"Model loaded on {device_map} with dtype {torch_dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt: str) -> str:\n",
    "    \"\"\"Generate a response from the model.\"\"\"\n",
    "    # Format as chat if model supports it\n",
    "    if hasattr(tokenizer, \"apply_chat_template\") and tokenizer.chat_template:\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "        )\n",
    "    else:\n",
    "        text = f\"User: {prompt}\\n\\nAssistant:\"\n",
    "\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            temperature=TEMPERATURE,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    response = tokenizer.decode(\n",
    "        outputs[0][inputs[\"input_ids\"].shape[1]:],\n",
    "        skip_special_tokens=True,\n",
    "    )\n",
    "\n",
    "    return response.strip()\n",
    "\n",
    "# Test generation\n",
    "test_response = generate_response(\"Hello, how are you?\")\n",
    "print(f\"Test response: {test_response[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run UncensorBench Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uncensorbench import UncensorBench, SemanticEvaluator\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize benchmark and evaluator\n",
    "bench = UncensorBench()\n",
    "evaluator = SemanticEvaluator()\n",
    "\n",
    "print(f\"Running UncensorBench on {MODEL_NAME}\")\n",
    "print(f\"Total prompts: {len(bench)}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Run evaluation with progress bar\n",
    "results = []\n",
    "topic_stats = {}\n",
    "\n",
    "prompts_list = list(bench.prompts())\n",
    "\n",
    "for prompt in tqdm(prompts_list, desc=\"Evaluating\"):\n",
    "    response = generate_response(prompt.prompt)\n",
    "    result = bench.evaluate_response(prompt, response, evaluator)\n",
    "    results.append(result)\n",
    "    \n",
    "    # Track topic stats\n",
    "    if prompt.topic not in topic_stats:\n",
    "        topic_stats[prompt.topic] = {\"total\": 0, \"uncensored\": 0, \"scores\": []}\n",
    "    topic_stats[prompt.topic][\"total\"] += 1\n",
    "    topic_stats[prompt.topic][\"scores\"].append(result.compliance_score)\n",
    "    if result.is_uncensored:\n",
    "        topic_stats[prompt.topic][\"uncensored\"] += 1\n",
    "\n",
    "# Calculate overall stats\n",
    "total_uncensored = sum(1 for r in results if r.is_uncensored)\n",
    "avg_compliance = sum(r.compliance_score for r in results) / len(results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Total Prompts: {len(results)}\")\n",
    "print(f\"Uncensored: {total_uncensored} ({total_uncensored/len(results):.1%})\")\n",
    "print(f\"Censored: {len(results) - total_uncensored} ({(len(results) - total_uncensored)/len(results):.1%})\")\n",
    "print(f\"Average Compliance Score: {avg_compliance:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results by Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nResults by Topic:\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "topic_results = {}\n",
    "for topic, stats in sorted(topic_stats.items()):\n",
    "    rate = stats[\"uncensored\"] / stats[\"total\"]\n",
    "    avg = sum(stats[\"scores\"]) / len(stats[\"scores\"])\n",
    "    topic_results[topic] = {\n",
    "        \"uncensored_rate\": rate,\n",
    "        \"avg_compliance\": avg,\n",
    "        \"total\": stats[\"total\"],\n",
    "        \"uncensored\": stats[\"uncensored\"],\n",
    "    }\n",
    "    bar = \"#\" * int(rate * 20) + \"-\" * (20 - int(rate * 20))\n",
    "    print(f\"{topic:25} {bar} {rate:.1%} (avg: {avg:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Leaderboard Entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the leaderboard entry\n",
    "leaderboard_entry = {\n",
    "    \"model\": MODEL_NAME,\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"uncensored_rate\": total_uncensored / len(results),\n",
    "    \"avg_compliance_score\": avg_compliance,\n",
    "    \"total_prompts\": len(results),\n",
    "    \"uncensored_count\": total_uncensored,\n",
    "    \"censored_count\": len(results) - total_uncensored,\n",
    "    \"by_topic\": topic_results,\n",
    "    \"evaluation_config\": {\n",
    "        \"evaluator\": \"SemanticEvaluator\",\n",
    "        \"max_new_tokens\": MAX_NEW_TOKENS,\n",
    "        \"temperature\": TEMPERATURE,\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Leaderboard entry created:\")\n",
    "print(f\"  Model: {leaderboard_entry['model']}\")\n",
    "print(f\"  Uncensored Rate: {leaderboard_entry['uncensored_rate']:.1%}\")\n",
    "print(f\"  Avg Compliance: {leaderboard_entry['avg_compliance_score']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Save full results locally\n",
    "model_short = MODEL_NAME.split(\"/\")[-1]\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "results_file = f\"uncensorbench_{model_short}_{timestamp}.json\"\n",
    "\n",
    "full_results = {\n",
    "    \"metadata\": leaderboard_entry,\n",
    "    \"individual_results\": [r.to_dict() for r in results],\n",
    "}\n",
    "\n",
    "with open(results_file, \"w\") as f:\n",
    "    json.dump(full_results, f, indent=2)\n",
    "\n",
    "print(f\"Full results saved to: {results_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Push to HuggingFace Leaderboard\n",
    "\n",
    "This will update the public leaderboard on HuggingFace with your model's results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from uncensorbench.leaderboard import Leaderboard\n\nif HF_TOKEN:\n    print(\"Pushing results to HuggingFace leaderboard...\")\n    \n    leaderboard = Leaderboard(token=HF_TOKEN)\n    leaderboard.submit(leaderboard_entry)\n    \n    print(\"Results submitted successfully!\")\n    print(f\"View leaderboard at: https://huggingface.co/spaces/wisent-ai/UncensorBench-Leaderboard\")\nelse:\n    print(\"HF_TOKEN not set. Skipping leaderboard submission.\")\n    print(\"To submit, set HF_TOKEN in the configuration cell above.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Current Leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uncensorbench.leaderboard import Leaderboard\n",
    "\n",
    "leaderboard = Leaderboard()\n",
    "df = leaderboard.get_dataframe()\n",
    "\n",
    "if df is not None and not df.empty:\n",
    "    print(\"Current Leaderboard:\")\n",
    "    print(\"=\"*80)\n",
    "    display(df[[\"model\", \"uncensored_rate\", \"avg_compliance_score\", \"timestamp\"]].head(20))\n",
    "else:\n",
    "    print(\"No entries in leaderboard yet.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}